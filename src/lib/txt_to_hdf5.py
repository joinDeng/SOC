#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠŠ space_object/<ID>/ncf/*.txt ä¸ orbit/*.txt åˆå¹¶æˆå•ä¸ª HDF5 æ–‡ä»¶
> python txt2hdf5.py --root /data/space_object --out space.h5
"""
import os
import json
import argparse
import h5py
import pandas as pd
import numpy as np
from datetime import datetime
from tqdm import tqdm
from pathos.pools import ProcessPool as Pool

DATE_FMT = "%Y-%m-%dT%H:%M:%S.%f"


def parse_one_object(dir_name: str, root: str):
    """
    dir_name: ä¾‹å¦‚ '12345_payload'
    è¿”å› dict or None
    """
    try:
        # ----   1. æ‹†åˆ† ID ä¸æ ‡ç­¾ ----
        norad_id, label = dir_name.split('_', 1)  # åªåˆ‡ä¸€æ¬¡ï¼Œé˜²æ­¢æ ‡ç­¾é‡Œè¿˜æœ‰ä¸‹åˆ’çº¿
        obj_types = ['payload', 'rocket body', 'debris']
        rcs_sizes = ['small', 'medium', 'large']
        try:
            cat_label = obj_types[int(label) // 3]
            rcs_label = rcs_sizes[int(label) % 3]
        except TypeError as e:
            print(e, 'in label')

        ncf_dir   = os.path.join(root, dir_name, "ncf")
        orbit_dir = os.path.join(root, dir_name, "orbit")
        ncf_txt   = [f for f in os.listdir(ncf_dir)   if f.endswith(".txt")][0]
        orbit_txt = [f for f in os.listdir(orbit_dir) if f.endswith(".txt")][0]
        ncf_path  = os.path.join(ncf_dir, ncf_txt)
        orbit_path= os.path.join(orbit_dir, orbit_txt)

        # ---- 2. è¯»å–ã€å¯¹é½ï¼ˆåŒæ—§è„šæœ¬ï¼‰ ----
        ncf_df  = pd.read_csv(ncf_path,  header=None, names=["datetime","ncf_x","ncf_y","ncf_z"], skiprows=1, sep='\s*,\s*', engine='python')
        orbit_df= pd.read_csv(orbit_path,header=None, names=["datetime","pos_x","pos_y","pos_z","vel_x","vel_y","vel_z"], skiprows=1, sep='\s*,\s*', engine='python')
        ncf_df["datetime"]  = pd.to_datetime(ncf_df["datetime"], format=DATE_FMT)
        orbit_df["datetime"]= pd.to_datetime(orbit_df["datetime"], format=DATE_FMT)
        df = pd.merge(ncf_df, orbit_df, on="datetime", how="inner")
        if len(df) < 30:
            return None

        # ---- 3. æ‰“åŒ… ----
        t   = df["datetime"].astype(np.int64)//10**9
        pos = df[["pos_x","pos_y","pos_z"]].values
        vel = df[["vel_x","vel_y","vel_z"]].values
        ncf = df[["ncf_x","ncf_y","ncf_z"]].values
        return {
            "norad_id": norad_id,
            "t": t,
            "pos": pos,
            "vel": vel,
            "ncf": ncf
        }
    except Exception as e:
        print(f"[ERROR] {dir_name}: {e}")
        return None


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--root", type=str, required=True, help="space_objectæ ¹ç›®å½•")
    parser.add_argument("--out", type=str, default="space.h5", help="è¾“å‡ºHDF5æ–‡ä»¶")
    args = parser.parse_args()

    # ---- 1. æ‰«ææ‰€æœ‰ NORAD ID ----
    root = args.root
    all_ids = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]
    print(f"[INFO] æ‰¾åˆ° {len(all_ids)} ä¸ª NORAD ID")

    # ---- 2. å¤šè¿›ç¨‹è§£æ ----
    with Pool() as pool:
        results = list(tqdm(pool.map(lambda x: parse_one_object(x, root), all_ids), total=len(all_ids)))
    pool.close()
    pool.join()

    # ---- 3. è¿‡æ»¤å¤±è´¥çš„ ----
    results = [r for r in results if r is not None]
    print(f"[INFO] æˆåŠŸè§£æ {len(results)} ä¸ªç›®æ ‡")

    # ---- 4. å†™å…¥ HDF5 ----
    with h5py.File(args.out, "w") as h5f:
        for res in tqdm(results):
            if res is None:
                continue
            norad_id = res["norad_id"]
            grp = h5f.create_group(norad_id)
            grp.attrs["label"] = res["label"]  # ğŸ”§ æŠŠç±»åˆ«å­˜æˆå±æ€§
            grp.create_dataset("t", data=res["t"], compression="gzip", compression_opts=9)
            grp.create_dataset("pos", data=res["pos"], compression="gzip", compression_opts=9)
            grp.create_dataset("vel", data=res["vel"], compression="gzip", compression_opts=9)
            grp.create_dataset("ncf", data=res["ncf"], compression="gzip", compression_opts=9)

# def main():
#     # ---- 1. æ‰«ææ‰€æœ‰ NORAD ID ----
#     root = "D:\\app\\program\\Python\\Pycharm\\project\\SOC\\data"
#     all_ids = [d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))]
#     print(f"[INFO] æ‰¾åˆ° {len(all_ids)} ä¸ª NORAD ID")
#
#     # ---- 2. å¤šè¿›ç¨‹è§£æ ----
#     with Pool() as pool:
#         results = list(tqdm(pool.map(lambda x: parse_one_object(x, root), all_ids), total=len(all_ids)))
#     pool.close()
#     pool.join()
#
#     # ---- 3. è¿‡æ»¤å¤±è´¥çš„ ----
#     results = [r for r in results if r is not None]
#     print(f"[INFO] æˆåŠŸè§£æ {len(results)} ä¸ªç›®æ ‡")
#
#     # ---- 4. å†™å…¥ HDF5 ----
#     out = "D:\\app\\program\\Python\\Pycharm\\project\\SOC\\dataset\\space.h5"
#     with h5py.File(out, "w") as h5f:
#         for res in tqdm(results):
#             if res is None:
#                 continue
#             norad_id = res["norad_id"]
#             grp = h5f.create_group(norad_id)
#             grp.attrs["label"] = res["label"]  # ğŸ”§ æŠŠç±»åˆ«å­˜æˆå±æ€§
#             grp.create_dataset("t", data=res["t"], compression="gzip", compression_opts=9)
#             grp.create_dataset("pos", data=res["pos"], compression="gzip", compression_opts=9)
#             grp.create_dataset("vel", data=res["vel"], compression="gzip", compression_opts=9)
#             grp.create_dataset("ncf", data=res["ncf"], compression="gzip", compression_opts=9)


if __name__ == "__main__":
    main()
